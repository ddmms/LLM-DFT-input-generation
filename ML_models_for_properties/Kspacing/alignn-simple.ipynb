{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b107025f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import json\n",
    "import shutil\n",
    "from random import sample\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import MultiStepLR, StepLR\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler, SequentialSampler\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, matthews_corrcoef\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "\n",
    "import pytorch_lightning as L\n",
    "from pytorch_lightning.loggers.csv_logs import CSVLogger\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, StochasticWeightAveraging\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning import Trainer\n",
    "from torchmetrics.functional import mean_squared_error, mean_absolute_error\n",
    "from pymatgen.core.composition import Composition\n",
    "from pymatgen.core.structure import Structure\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch.nn import L1Loss, MSELoss, HuberLoss\n",
    "\n",
    "data_type_np = np.float32\n",
    "data_type_torch = torch.float32\n",
    "\n",
    "# import wandb\n",
    "\n",
    "import dgl\n",
    "from jarvis.db.jsonutils import loadjson\n",
    "import pickle as pk\n",
    "import pandas as pd\n",
    "import lmdb\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "from jarvis.core.atoms import Atoms\n",
    "from jarvis.core.graphs import Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0649c629",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "\n",
    "The data should be formated similar to CGCNN: individual structures are represented by CIF files in the forlder data_dir=\"alignn/examples/sample_data/\", also there should be id_prop.csv with two columns: first is the name of the file (not just id like in CGCNN), the second column is property (klength in our case). Also the data_dir should contain config_example.json compatible with ALIGNNConfig (if you download alignn repository from github, it will contain alignn_atomwise config file which is different, so take care about it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5aca610",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2baa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple\n",
    "from alignn.lmdb_dataset import prepare_line_graph_batch\n",
    "\n",
    "class TorchLMDBDataset(Dataset):\n",
    "    \"\"\"Dataset of crystal DGLGraphs using LMDB.\"\"\"\n",
    "\n",
    "    def __init__(self, lmdb_path=\"\", line_graph=True):\n",
    "        \"\"\"Intitialize with path and ids array.\"\"\"\n",
    "        super(TorchLMDBDataset, self).__init__()\n",
    "        self.lmdb_path = lmdb_path\n",
    "        self.line_graph = line_graph\n",
    "        self.env = lmdb.open(self.lmdb_path, readonly=True, lock=False)\n",
    "        with self.env.begin() as txn:\n",
    "            self.length = txn.stat()[\"entries\"]\n",
    "        self.prepare_batch = prepare_line_graph_batch\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Get length.\"\"\"\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get sample.\"\"\"\n",
    "        with self.env.begin() as txn:\n",
    "            serialized_data = txn.get(f\"{idx}\".encode())\n",
    "        if self.line_graph:\n",
    "            graph, line_graph, label, ids = pk.loads(serialized_data)\n",
    "            return graph, line_graph, label, ids\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Close connection.\"\"\"\n",
    "        self.env.close()\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Delete connection.\"\"\"\n",
    "        self.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def collate_line_graph(\n",
    "        samples: List[Tuple[dgl.DGLGraph, dgl.DGLGraph, torch.Tensor, torch.Tensor]]\n",
    "    ):\n",
    "        \"\"\"Dataloader helper to batch graphs cross `samples`.\"\"\"\n",
    "        graphs, line_graphs, labels, ids = map(list, zip(*samples))\n",
    "        batched_graph = dgl.batch(graphs)\n",
    "        batched_line_graph = dgl.batch(line_graphs)\n",
    "        if len(labels[0].size()) > 0:\n",
    "            return batched_graph, batched_line_graph, torch.stack(labels), torch.stack(ids)\n",
    "        else:\n",
    "            return batched_graph, batched_line_graph, torch.tensor(labels), torch.tensor(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c39f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=\"alignn/examples/sample_data/\"\n",
    "config_name=os.path.join(data_dir,\"config_example.json\")\n",
    "id_prop_csv=os.path.join(data_dir,\"id_prop.csv\")\n",
    "output_dir=\"alignn/output/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "421af08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lmdb_database(data, file_path, data_dir):\n",
    "    env = lmdb.open(os.path.join(data_dir,file_path), map_size=int(1e12))\n",
    "    with env.begin(write=True) as txn:\n",
    "        for idx in range(len(data)):\n",
    "            structure = Structure.from_file(os.path.join(data_dir, data.iloc[idx][0]))\n",
    "            structure_dict={\n",
    "                    'lattice_mat': structure.lattice.matrix,\n",
    "                    'coords': [site.frac_coords for site in structure.sites],\n",
    "                    'elements': [str(site.specie) for site in structure.sites],\n",
    "                    'abc': structure.lattice.abc,\n",
    "                    'angles': structure.lattice.angles,\n",
    "                    'cartesian': False,\n",
    "                    'props': ['' for site in structure.sites]\n",
    "                   }\n",
    "            atoms=Atoms.from_dict(structure_dict)\n",
    "            g, lg = Graph.atom_dgl_multigraph(\n",
    "                        atoms,\n",
    "                        cutoff=float(15),\n",
    "                        max_neighbors=12,\n",
    "                        atom_features=\"cgcnn\",\n",
    "                        compute_line_graph=True,\n",
    "                        use_canonize=False,\n",
    "                        neighbor_strategy=\"k-nearest\",\n",
    "                    )\n",
    "            label = torch.tensor(data.iloc[idx][1]).type(torch.get_default_dtype())\n",
    "            ids=torch.tensor(int(data.iloc[idx][0][:-4]))\n",
    "            serialized_data = pk.dumps((g, lg, label, ids))\n",
    "            txn.put(f\"{idx}\".encode(), serialized_data)\n",
    "    env.close()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d60d57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ALIGNNDataModule(L.LightningDataModule):\n",
    "    def __init__(self, root_dir: str,\n",
    "                 id_prop_csv: str,\n",
    "                 train_ratio = 0.8,\n",
    "                 val_ratio = 0.1, \n",
    "                 test_ratio = 0.1,\n",
    "                 lmdb_exist = False,\n",
    "                 batch_size = 64,\n",
    "                 num_workers = 0,\n",
    "                 pin_memory = True,\n",
    "                 random_seed = 123):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.random_seed=random_seed\n",
    "        self.batch_size = batch_size\n",
    "        self.pin_memory = pin_memory\n",
    "        self.num_workers=num_workers\n",
    "        \n",
    "        data=pd.read_csv(id_prop_csv,header=None)\n",
    "        train_idx, test_idx = train_test_split(data.index.values, test_size=test_ratio, random_state=123,)\n",
    "        train_idx, val_idx = train_test_split(train_idx, train_size=train_ratio/(1-test_ratio), random_state=123)\n",
    "        \n",
    "        train=data.iloc[train_idx]\n",
    "        train.reset_index(inplace=True, drop=True)\n",
    "        train.to_csv(os.path.join(root_dir,'train.csv'))\n",
    "        \n",
    "        val=data.iloc[val_idx]\n",
    "        val.reset_index(inplace=True, drop=True)\n",
    "        val.to_csv(os.path.join(root_dir,'val.csv'))\n",
    "        \n",
    "        test=data.iloc[test_idx]\n",
    "        test.reset_index(inplace=True, drop=True)\n",
    "        test.to_csv(os.path.join(root_dir,'test.csv'))\n",
    "\n",
    "        list_of_paths=['train_data.lmdb', 'test_data.lmdb', 'val_data.lmdb']\n",
    "        \n",
    "        if(lmdb_exist == False):\n",
    "            if os.path.exists(os.path.join(root_dir,'train_data.lmdb')):\n",
    "                shutil.rmtree(os.path.join(root_dir,'train_data.lmdb'))\n",
    "            if os.path.exists(os.path.join(root_dir,'val_data.lmdb')):\n",
    "                shutil.rmtree(os.path.join(root_dir,'val_data.lmdb'))\n",
    "            if os.path.exists(os.path.join(root_dir,'test_data.lmdb')):\n",
    "                shutil.rmtree(os.path.join(root_dir,'test_data.lmdb'))\n",
    "            create_lmdb_database(train,'train_data.lmdb', root_dir)\n",
    "            create_lmdb_database(val,'val_data.lmdb', root_dir)\n",
    "            create_lmdb_database(test,'test_data.lmdb', root_dir)  \n",
    "        elif not all(os.path.exists(os.path.join(root_dir,var)) for var in list_of_paths):\n",
    "            print(\"Put lmdb_exist to False or provide train/val/test lmdb files.\")\n",
    "            \n",
    "        self.train_dataset=TorchLMDBDataset(lmdb_path=os.path.join(root_dir,'train_data.lmdb'), line_graph=True)\n",
    "        self.val_dataset=TorchLMDBDataset(lmdb_path=os.path.join(root_dir,'val_data.lmdb'), line_graph=True)\n",
    "        self.test_dataset=TorchLMDBDataset(lmdb_path=os.path.join(root_dir,'test_data.lmdb'), line_graph=True)\n",
    "\n",
    "        self.train_collate=self.train_dataset.collate_line_graph\n",
    "        self.val_collate=self.val_dataset.collate_line_graph\n",
    "        self.test_collate=self.test_dataset.collate_line_graph\n",
    "  \n",
    "    def train_dataloader(self,shuffle=True):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers,collate_fn=self.train_collate, \n",
    "                          pin_memory=self.pin_memory, shuffle=shuffle)\n",
    "    def val_dataloader(self,shuffle=False):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, collate_fn=self.val_collate, \n",
    "                          pin_memory=self.pin_memory, shuffle=shuffle)\n",
    "    def test_dataloader(self,shuffle=False):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, collate_fn=self.test_collate, \n",
    "                          pin_memory=self.pin_memory, shuffle=shuffle)\n",
    "    def predict_dataloader(self,shuffle=False):\n",
    "        return DataLoader(self.test_dataset, batch_size=self.batch_size,\n",
    "                          num_workers=self.num_workers, collate_fn=self.test_collate, \n",
    "                          pin_memory=self.pin_memory, shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e947195",
   "metadata": {},
   "outputs": [],
   "source": [
    "config=loadjson(config_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "443a0398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from alignn.models.alignn import ALIGNN, ALIGNNConfig\n",
    "\n",
    "class ALIGNNLightning(L.LightningModule):\n",
    "    def __init__(self, **config):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        L.seed_everything(config['random_seed'])\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        tmp=ALIGNNConfig(**config['model'])\n",
    "        self.model=ALIGNN(tmp)\n",
    "        print(f'Model size: {count_parameters(self.model)} parameters\\n')\n",
    "\n",
    "        ### here we define some important parameters\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.decay = config['weight_decay']\n",
    "        \n",
    "        ### we also define loss function based on task\n",
    "        self.criterion = HuberLoss()\n",
    "\n",
    "    def forward(self, graph_list):\n",
    "        out=self.model(graph_list)\n",
    "        return out\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(model.parameters(), self.learning_rate,\n",
    "                              weight_decay=self.decay)\n",
    "        # lr_scheduler=StepLR(optimizer,\n",
    "        #                     step_size=1,\n",
    "        #                     gamma=0.5)\n",
    "        \n",
    "        # return [optimizer], [lr_scheduler]\n",
    "        return [optimizer]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        g, lg, labels, ids = batch\n",
    "        output = self([g,lg])\n",
    "       \n",
    "        loss = self.criterion(output, labels)\n",
    "        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        mse = mean_squared_error(labels.cpu(), output.cpu())\n",
    "        mae = mean_absolute_error(labels.cpu(), output.cpu())\n",
    "        self.log(\"train_mse\", float(mse), on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        self.log(\"train_mae\", float(mae), on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        g, lg, labels, ids = batch\n",
    "        output = self([g,lg])\n",
    "        \n",
    "        loss = self.criterion(output, labels)\n",
    "        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        \n",
    "        mse = mean_squared_error(labels.cpu(), output.cpu())\n",
    "        mae = mean_absolute_error(labels.cpu(), output.cpu())\n",
    "        self.log(\"val_mse\", float(mse), on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        self.log(\"val_mae\", float(mae), on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        return loss\n",
    "     \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        g, lg, labels, ids = batch\n",
    "        output = self([g,lg])\n",
    "        \n",
    "        loss = self.criterion(output, labels)\n",
    "        \n",
    "        mse = mean_squared_error(labels.cpu(), output.cpu())\n",
    "        mae = mean_absolute_error(labels.cpu(), output.cpu())\n",
    "        self.log(\"test_mse\", float(mse), on_step=False, on_epoch=True, prog_bar=False, logger=True, batch_size=self.batch_size)\n",
    "        self.log(\"test_mae\", float(mae), on_step=False, on_epoch=True, prog_bar=True, logger=True, batch_size=self.batch_size)\n",
    "        return \n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        g, lg, labels, ids = batch\n",
    "        output = self([g,lg])\n",
    "        \n",
    "        mse = mean_squared_error(labels.cpu(), output.cpu())\n",
    "        mae = mean_absolute_error(labels.cpu(), output.cpu())\n",
    "        return output.cpu(), labels.cpu(), ids.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13bec7ed-1785-4239-8760-e89baa250e88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = ALIGNNDataModule(root_dir = data_dir, \n",
    "                        id_prop_csv = id_prop_csv, \n",
    "                        lmdb_exist = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d28d665",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 4026753 parameters\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = ALIGNNLightning(**config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "079cd8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/xyq44482/micromamba/envs/alignn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=1000,accelerator='gpu', devices=1, \n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=50), \n",
    "                             ModelCheckpoint(monitor='val_mae', mode=\"min\", \n",
    "                                dirpath='alignn_models/alignn_trained_models/', filename='alignn-{epoch:02d}-{val_acc:.2f}')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68acebeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA A100 80GB PCIe') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/xyq44482/micromamba/envs/alignn/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/xyq44482/Documents/Uncertainty-quntification/alignn_models/alignn_trained_models exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type      | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model     | ALIGNN    | 4.0 M  | train\n",
      "1 | criterion | HuberLoss | 0      | train\n",
      "------------------------------------------------\n",
      "4.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.0 M     Total params\n",
      "16.107    Total estimated model params size (MB)\n",
      "136       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88b16c1ed58e4506b562541a85d03707",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xyq44482/micromamba/envs/alignn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n",
      "/home/xyq44482/micromamba/envs/alignn/lib/python3.11/site-packages/torch/storage.py:414: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(io.BytesIO(b))\n",
      "/home/xyq44482/micromamba/envs/alignn/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e991d3bb79b24541ae1b1112422bb95d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                               | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68a9e91c64054bb79f0169f33a1b71b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4d479178274d558cc7d0b35b80eafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4885e9d651cf4cb5a028be53089dc901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d2e6f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/xyq44482/micromamba/envs/llm/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "Restoring states from the checkpoint path at alignn_models/alignn_trained_models/alignn.ckpt\n",
      "Loaded model weights from the checkpoint at alignn_models/alignn_trained_models/alignn.ckpt\n",
      "/home/xyq44482/micromamba/envs/llm/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=27` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72e25d78fe304081bfa42a43659265b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                             | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = Trainer(max_epochs=1000,accelerator='cpu', devices=1, \n",
    "                  callbacks=[EarlyStopping(monitor='val_loss', patience=50), \n",
    "                             ModelCheckpoint(monitor='val_mae', mode=\"min\", \n",
    "                                dirpath='alignn_models/alignn_trained_models/', filename='alignn-{epoch:02d}-{val_acc:.2f}')])\n",
    "\n",
    "pred = trainer.predict(model, ckpt_path=\"alignn_models/alignn_trained_models/alignn.ckpt\", datamodule=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035d12cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_train = trainer.predict(model, data.train_dataloader(),ckpt_path=\"cgcnn_models/cgcnn_trained_models/k_number.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fe538b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val = trainer.predict(model, data.val_dataloader(),ckpt_path=\"cgcnn_models/cgcnn_trained_models/k_number.ckpt\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017f10c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8140afdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Graph(num_nodes=61, num_edges=1508,\n",
      "      ndata_schemes={'atom_features': Scheme(shape=(92,), dtype=torch.float32)}\n",
      "      edata_schemes={'r': Scheme(shape=(3,), dtype=torch.float32)}), Graph(num_nodes=1508, num_edges=37506,\n",
      "      ndata_schemes={'r': Scheme(shape=(3,), dtype=torch.float32)}\n",
      "      edata_schemes={'h': Scheme(shape=(), dtype=torch.float32)}), tensor([30.0000, 45.0000, 57.5000, 71.2500, 60.0000, 45.0000, 32.5000, 25.0000,\n",
      "        65.0000, 20.0000]), tensor([54784, 41010, 57391, 59819, 60833, 38078, 60814, 44557, 20725, 15453])]\n"
     ]
    }
   ],
   "source": [
    "num_batches=0\n",
    "for batch in data.train_dataloader():\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e71dedbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e176574",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e9dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a404220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd943647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18210372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5afa312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85886ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f4c2de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fcf447",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19692ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac690cf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08881c23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d38154",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7248cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855cfbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a5f0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fde2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0d41b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38756819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd14ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d21055d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d870f41e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2895ade8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8fada0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212837c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9d6409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb0de4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa58423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd049f1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8dfc066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9428b692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96accb8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627ecca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0124a14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c20dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eedf74a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5fb391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a1d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778c9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48102931",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
