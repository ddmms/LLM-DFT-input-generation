{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "SQoQqglcJUOq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformer\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q datasets\n",
        "!pip install -q accelerate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-um9DiKJj8X",
        "outputId": "8ad2858d-8042-4ab9-ea2d-bf43707f33d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformer (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformer\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\""
      ],
      "metadata": {
        "id": "ZIK8vCjegDIW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definititon of dataclasses: <br>\n",
        "ModelArguments, <br>\n",
        "DataArguments, <br>\n",
        "TrainingArguments, <br>\n",
        "GenerationArguments, <br>\n",
        "DataCollatorForCausalLM"
      ],
      "metadata": {
        "id": "AGRNr9DEPE6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "import transformers\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch\n",
        "\n",
        "@dataclass\n",
        "class ModelArguments:\n",
        "    model_name_or_path: Optional[str] = field(\n",
        "        default=\"EleutherAI/pythia-12b\"\n",
        "    )\n",
        "    trust_remote_code: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Enable unpickling of arbitrary code in AutoModelForCausalLM#from_pretrained.\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class DataArguments:\n",
        "    eval_dataset_size: int = field(\n",
        "        default=1024, metadata={\"help\": \"Size of validation dataset.\"}\n",
        "    )\n",
        "    max_train_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    max_eval_samples: Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\n",
        "            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n",
        "            \"value if set.\"\n",
        "        },\n",
        "    )\n",
        "    source_max_len: int = field(\n",
        "        default=4096,\n",
        "        metadata={\"help\": \"Maximum source sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "    target_max_len: int = field(\n",
        "        default=1024,\n",
        "        metadata={\"help\": \"Maximum target sequence length. Sequences will be right padded (and possibly truncated).\"},\n",
        "    )\n",
        "    dataset: str = field(\n",
        "        default='alpaca',\n",
        "        metadata={\"help\": \"Which dataset to finetune on. See datamodule for options.\"}\n",
        "    )\n",
        "    dataset_format: Optional[str] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Which dataset format is used. [alpaca|chip2|self-instruct|hh-rlhf]\"}\n",
        "    )\n",
        "\n",
        "@dataclass\n",
        "class TrainingArguments(transformers.Seq2SeqTrainingArguments):\n",
        "\n",
        "    train_on_source: Optional[bool] = field(\n",
        "        default=False,\n",
        "        metadata={\"help\": \"Whether to train on the input in addition to the target text.\"}\n",
        "    )\n",
        "    report_to: str = field(\n",
        "        default='none',\n",
        "        metadata={\"help\": \"To use wandb or something else for reporting.\"}\n",
        "    )\n",
        "    output_dir: str = field(default='./output', metadata={\"help\": 'The output dir for logs and checkpoints'})\n",
        "    optim: str = field(default='adamw_torch', metadata={\"help\": 'The optimizer to be used'})\n",
        "    per_device_train_batch_size: int = field(default=16, metadata={\"help\": 'The training batch size per GPU. Increase for better speed.'})\n",
        "    gradient_accumulation_steps: int = field(default=1, metadata={\"help\": 'How many gradients to accumulate before to perform an optimizer step'})\n",
        "    max_steps: int = field(default=10000, metadata={\"help\": 'How many optimizer update steps to take'})\n",
        "    weight_decay: float = field(default=0.0, metadata={\"help\": 'The L2 weight decay rate of AdamW'})\n",
        "    learning_rate: float = field(default=0.0002, metadata={\"help\": 'The learnign rate'})\n",
        "    remove_unused_columns: bool = field(default=False, metadata={\"help\": 'Removed unused columns. Needed to make this codebase work.'})\n",
        "    max_grad_norm: float = field(default=0.3, metadata={\"help\": 'Gradient clipping max norm. This is tuned and works well for all models tested.'})\n",
        "    gradient_checkpointing: bool = field(default=True, metadata={\"help\": 'Use gradient checkpointing. You want to use this.'})\n",
        "    do_train: bool = field(default=True, metadata={\"help\": 'To train or not to train, that is the question?'})\n",
        "    lr_scheduler_type: str = field(default='constant', metadata={\"help\": 'Learning rate schedule. Constant a bit better than cosine, and has advantage for analysis'})\n",
        "    warmup_ratio: float = field(default=0.03, metadata={\"help\": 'Fraction of steps to do a warmup for'})\n",
        "    logging_steps: int = field(default=10, metadata={\"help\": 'The frequency of update steps after which to log the loss'})\n",
        "    group_by_length: bool = field(default=True, metadata={\"help\": 'Group sequences into batches with same length. Saves memory and speeds up training considerably.'})\n",
        "    save_strategy: str = field(default='steps', metadata={\"help\": 'When to save checkpoints'})\n",
        "    save_steps: int = field(default=250, metadata={\"help\": 'How often to save a model'})\n",
        "    save_total_limit: int = field(default=40, metadata={\"help\": 'How many checkpoints to save before the oldest is overwritten'})\n",
        "\n",
        "@dataclass\n",
        "class GenerationArguments:\n",
        "    # For more hyperparameters check:\n",
        "    # https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig\n",
        "    # Length arguments\n",
        "    max_new_tokens: Optional[int] = field(\n",
        "        default=1024,\n",
        "        metadata={\"help\": \"Maximum number of new tokens to be generated in evaluation or prediction loops\"\n",
        "                          \"if predict_with_generate is set.\"}\n",
        "    )\n",
        "    min_new_tokens : Optional[int] = field(\n",
        "        default=None,\n",
        "        metadata={\"help\": \"Minimum number of new tokens to generate.\"}\n",
        "    )\n",
        "\n",
        "    # Generation strategy\n",
        "    do_sample: Optional[bool] = field(default=False)\n",
        "    num_beams: Optional[int] = field(default=1)\n",
        "    num_beam_groups: Optional[int] = field(default=1)\n",
        "    penalty_alpha: Optional[float] = field(default=None)\n",
        "    use_cache: Optional[bool] = field(default=True)\n",
        "\n",
        "    # Hyperparameters for logit manipulation\n",
        "    temperature: Optional[float] = field(default=1.0)\n",
        "    top_k: Optional[int] = field(default=50)\n",
        "    top_p: Optional[float] = field(default=1.0)\n",
        "    typical_p: Optional[float] = field(default=1.0)\n",
        "    diversity_penalty: Optional[float] = field(default=0.0)\n",
        "    repetition_penalty: Optional[float] = field(default=1.0)\n",
        "    length_penalty: Optional[float] = field(default=1.0)\n",
        "    no_repeat_ngram_size: Optional[int] = field(default=0)\n",
        "\n",
        "\n",
        "# here it is not clear where is the prompt which is supposed to be included in\n",
        "# a scource sentence.\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM(object):\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "    source_max_len: int\n",
        "    target_max_len: int\n",
        "    train_on_source: bool\n",
        "    predict_with_generate: bool\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract elements\n",
        "        sources = [f\"{self.tokenizer.bos_token}{example['input']}\" for example in instances]\n",
        "        targets = [f\"{example['output']}{self.tokenizer.eos_token}\" for example in instances]\n",
        "        # Tokenize\n",
        "        tokenized_sources_with_prompt = self.tokenizer( # ? where is the prompt ?\n",
        "            sources,\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        tokenized_targets = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        # Build the input and labels for causal LM\n",
        "        input_ids = []\n",
        "        labels = []\n",
        "        for tokenized_source, tokenized_target in zip(\n",
        "            tokenized_sources_with_prompt['input_ids'],\n",
        "            tokenized_targets['input_ids']\n",
        "        ):\n",
        "            if not self.predict_with_generate:\n",
        "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
        "                if not self.train_on_source:\n",
        "                    labels.append(\n",
        "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
        "                    )\n",
        "                else:\n",
        "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
        "            else:\n",
        "                input_ids.append(torch.tensor(tokenized_source))\n",
        "        # Apply padding\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
        "        data_dict = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        }\n",
        "        if labels is not None:\n",
        "            data_dict['labels'] = labels\n",
        "        return data_dict"
      ],
      "metadata": {
        "id": "c7J2dmWpO_eG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DataPreparation <br>\n",
        "\n",
        "The main functions which we need is local_dataset, make_data_module <br>\n",
        "Datasets are expected to have the following columns: { `input`, `output` }"
      ],
      "metadata": {
        "id": "VhqjFUHaZIeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "def extract_unnatural_instructions_data(examples, extract_reformulations=False):\n",
        "    out = {\n",
        "        'input': [],\n",
        "        'output': [],\n",
        "    }\n",
        "    for example_instances in examples['instances']:\n",
        "        for instance in example_instances:\n",
        "            out['input'].append(instance['instruction_with_input'])\n",
        "            out['output'].append(instance['output'])\n",
        "    if extract_reformulations:\n",
        "        for example_reformulations in examples['reformulations']:\n",
        "            if example_reformulations is not None:\n",
        "                for instance in example_reformulations:\n",
        "                    out['input'].append(instance['instruction_with_input'])\n",
        "                    out['output'].append(instance['output'])\n",
        "    return out\n",
        "\n",
        "ALPACA_PROMPT_DICT = {\n",
        "    \"prompt_input\": (\n",
        "        \"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response: \"\n",
        "    ),\n",
        "    \"prompt_no_input\": (\n",
        "        \"Below is an instruction that describes a task. \"\n",
        "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
        "        \"### Instruction:\\n{instruction}\\n\\n### Response: \"\n",
        "    ),\n",
        "}\n",
        "\n",
        "def extract_alpaca_dataset(example):\n",
        "    if example.get(\"input\", \"\") != \"\":\n",
        "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_input\"]\n",
        "    else:\n",
        "        prompt_format = ALPACA_PROMPT_DICT[\"prompt_no_input\"]\n",
        "    return {'input': prompt_format.format(**example)}\n",
        "\n",
        "### The main function which we need is local_dataset\n",
        "\n",
        "def local_dataset(dataset_name):\n",
        "    if dataset_name.endswith('.json') or dataset_name.endswith('.jsonl'):\n",
        "        full_dataset = Dataset.from_json(path_or_paths=dataset_name)\n",
        "    elif dataset_name.endswith('.csv'):\n",
        "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name))\n",
        "    elif dataset_name.endswith('.tsv'):\n",
        "        full_dataset = Dataset.from_pandas(pd.read_csv(dataset_name, delimiter='\\t'))\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported dataset format: {dataset_name}\")\n",
        "\n",
        "    split_dataset = full_dataset.train_test_split(test_size=0.1)\n",
        "    return split_dataset\n",
        "\n",
        "\n",
        "def make_data_module(tokenizer: transformers.PreTrainedTokenizer, args) -> Dict:\n",
        "    \"\"\"\n",
        "    Make dataset and collator for supervised fine-tuning.\n",
        "    Datasets are expected to have the following columns: { `input`, `output` }\n",
        "\n",
        "    Available datasets to be selected with `dataset` argument:\n",
        "        - alpaca, 52002 examples\n",
        "        - alpaca cleaned, 51942 examples\n",
        "        - chip2 (OIG), 210289 examples\n",
        "        - self-instruct, 82612 examples\n",
        "        - hh-rlhf (Anthropic), 160800 examples\n",
        "        - longform, 23.7k examples\n",
        "        - oasst1 (OpenAssistant) primary message tree only, 9,846 examples\n",
        "\n",
        "    Coming soon:\n",
        "        - unnatural instructions core, 66010 examples\n",
        "        - unnatural instructions full, 240670 examples\n",
        "        - alpaca-gpt4, 52002 examples\n",
        "        - unnatural-instructions-gpt4, 9000 examples\n",
        "        - supernatural-instructions, 69624 examples (same as paper with 100 ex/task more can be used)\n",
        "        - flan (FLAN v2), up to 20M examples available\n",
        "        - vicuna\n",
        "\n",
        "    \"\"\"\n",
        "    def load_data(dataset_name):\n",
        "        if dataset_name == 'alpaca':\n",
        "            return load_dataset(\"tatsu-lab/alpaca\")\n",
        "        elif dataset_name == 'alpaca-clean':\n",
        "            return load_dataset(\"yahma/alpaca-cleaned\")\n",
        "        elif dataset_name == 'chip2':\n",
        "            return load_dataset(\"laion/OIG\", data_files='unified_chip2.jsonl')\n",
        "        elif dataset_name == 'hh-rlhf':\n",
        "            return load_dataset(\"Anthropic/hh-rlhf\")\n",
        "        elif dataset_name == 'longform':\n",
        "            return load_dataset(\"akoksal/LongForm\")\n",
        "        elif dataset_name == 'oasst1':\n",
        "            return load_dataset(\"timdettmers/openassistant-guanaco\")\n",
        "        elif dataset_name == \"OpenAssistant/oasst_top1_2023-08-25\":\n",
        "            return load_dataset(\"OpenAssistant/oasst_top1_2023-08-25\")\n",
        "        elif dataset_name == 'vicuna':\n",
        "            raise NotImplementedError(\"Vicuna data was not released.\")\n",
        "        else:\n",
        "            if os.path.exists(dataset_name):\n",
        "                try:\n",
        "                    args.dataset_format = args.dataset_format if args.dataset_format else \"input-output\"\n",
        "                    full_dataset = local_dataset(dataset_name)\n",
        "                    return full_dataset\n",
        "                except:\n",
        "                    raise ValueError(f\"Error loading dataset from {dataset_name}\")\n",
        "            else:\n",
        "                raise NotImplementedError(f\"Dataset {dataset_name} not implemented yet.\")\n",
        "\n",
        "    def format_dataset(dataset, dataset_format):\n",
        "        if (\n",
        "            dataset_format == 'alpaca' or dataset_format == 'alpaca-clean' or\n",
        "            (dataset_format is None and args.dataset in ['alpaca', 'alpaca-clean'])\n",
        "        ):\n",
        "            dataset = dataset.map(extract_alpaca_dataset, remove_columns=['instruction'])\n",
        "        elif dataset_format == 'chip2' or (dataset_format is None and args.dataset == 'chip2'):\n",
        "            dataset = dataset.map(lambda x: {\n",
        "                'input': x['text'].split('\\n<bot>: ')[0].replace('<human>: ', ''),\n",
        "                'output': x['text'].split('\\n<bot>: ')[1],\n",
        "            })\n",
        "        elif dataset_format == 'self-instruct' or (dataset_format is None and args.dataset == 'self-instruct'):\n",
        "            for old, new in [[\"prompt\", \"input\"], [\"completion\", \"output\"]]:\n",
        "                dataset = dataset.rename_column(old, new)\n",
        "        elif dataset_format == 'hh-rlhf' or (dataset_format is None and args.dataset == 'hh-rlhf'):\n",
        "            dataset = dataset.map(lambda x: {\n",
        "                'input': '',\n",
        "                'output': x['chosen']\n",
        "            })\n",
        "        elif dataset_format == 'oasst1' or (dataset_format is None and args.dataset == 'oasst1'):\n",
        "            dataset = dataset.map(lambda x: {\n",
        "                'input': '',\n",
        "                'output': x['text'],\n",
        "            })\n",
        "        elif dataset_format == 'input-output':\n",
        "            # leave as is\n",
        "            pass\n",
        "        # Remove unused columns.\n",
        "        dataset = dataset.remove_columns(\n",
        "            [col for col in dataset.column_names['train'] if col not in ['input', 'output']]\n",
        "        )\n",
        "        return dataset\n",
        "\n",
        "\n",
        "    dataset = load_data(args.dataset)\n",
        "    dataset = format_dataset(dataset, args.dataset_format)\n",
        "\n",
        "    # Split train/eval, reduce size\n",
        "    if args.do_eval or args.do_predict:\n",
        "        if 'eval' in dataset:\n",
        "            eval_dataset = dataset['eval']\n",
        "        else:\n",
        "            print('Splitting train dataset in train and validation according to `eval_dataset_size`')\n",
        "            dataset = dataset[\"train\"].train_test_split(\n",
        "                test_size=args.eval_dataset_size, shuffle=True, seed=42\n",
        "            )\n",
        "            eval_dataset = dataset['test']\n",
        "        if args.max_eval_samples is not None and len(eval_dataset) > args.max_eval_samples:\n",
        "            eval_dataset = eval_dataset.select(range(args.max_eval_samples))\n",
        "        if args.group_by_length:\n",
        "            eval_dataset = eval_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "    if args.do_train:\n",
        "        train_dataset = dataset['train']\n",
        "        if args.max_train_samples is not None and len(train_dataset) > args.max_train_samples:\n",
        "            train_dataset = train_dataset.select(range(args.max_train_samples))\n",
        "        if args.group_by_length:\n",
        "            train_dataset = train_dataset.map(lambda x: {'length': len(x['input']) + len(x['output'])})\n",
        "\n",
        "    data_collator = DataCollatorForCausalLM(\n",
        "        tokenizer=tokenizer,\n",
        "        source_max_len=args.source_max_len,\n",
        "        target_max_len=args.target_max_len,\n",
        "        train_on_source=args.train_on_source,\n",
        "        predict_with_generate=args.predict_with_generate,\n",
        "    )\n",
        "    return dict(\n",
        "        train_dataset=train_dataset if args.do_train else None,\n",
        "        eval_dataset=eval_dataset if args.do_eval else None,\n",
        "        predict_dataset=eval_dataset if args.do_predict else None,\n",
        "        data_collator=data_collator\n",
        "    )"
      ],
      "metadata": {
        "id": "o1ko_oMIJlti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define training details"
      ],
      "metadata": {
        "id": "Evh8pNKseEQo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import isdir, join, exists\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed, Seq2SeqTrainer\n",
        "\n",
        "def get_accelerate_model(args, checkpoint_dir):\n",
        "    device_map = \"auto\"\n",
        "\n",
        "    # if we are in a distributed setting, we need to set the device map and max memory per device\n",
        "    if os.environ.get('LOCAL_RANK') is not None:\n",
        "        local_rank = int(os.environ.get('LOCAL_RANK', '0'))\n",
        "        device_map = {'': local_rank}\n",
        "\n",
        "\n",
        "    print(f'loading base model {args.model_name_or_path}...')\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        device_map=device_map,\n",
        "        trust_remote_code=args.trust_remote_code,\n",
        "    )\n",
        "  # Tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        padding_side=\"right\",\n",
        "        use_fast=True, # Fast tokenizer giving issues.\n",
        "        trust_remote_code=args.trust_remote_code,\n",
        "    )\n",
        "    if tokenizer._pad_token is None:\n",
        "        special_tokens_dict = dict(pad_token=DEFAULT_PAD_TOKEN)\n",
        "        if args.dataset == \"OpenAssistant/oasst_top1_2023-08-25\":\n",
        "            chat_special_tokens = [\"<|im_start|>\", \"<|im_end|>\"]\n",
        "            special_tokens_dict.update(additional_special_tokens=chat_special_tokens)\n",
        "\n",
        "        smart_tokenizer_and_embedding_resize(\n",
        "            special_tokens_dict=special_tokens_dict,\n",
        "            tokenizer=tokenizer,\n",
        "            model=model\n",
        "        )\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def smart_tokenizer_and_embedding_resize(\n",
        "    special_tokens_dict: Dict,\n",
        "    tokenizer: transformers.PreTrainedTokenizer,\n",
        "    model: transformers.PreTrainedModel,\n",
        "    non_special_tokens = None,\n",
        "):\n",
        "    \"\"\"Resize tokenizer and embedding.\n",
        "\n",
        "    Note: This is the unoptimized version that may make your embedding size not be divisible by 64.\n",
        "    \"\"\"\n",
        "    num_new_tokens = tokenizer.add_special_tokens(special_tokens_dict) + tokenizer.add_tokens(non_special_tokens)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    if num_new_tokens > 0:\n",
        "        input_embeddings_data = model.get_input_embeddings().weight.data\n",
        "        output_embeddings_data = model.get_output_embeddings().weight.data\n",
        "\n",
        "        input_embeddings_avg = input_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "        output_embeddings_avg = output_embeddings_data[:-num_new_tokens].mean(dim=0, keepdim=True)\n",
        "\n",
        "        input_embeddings_data[-num_new_tokens:] = input_embeddings_avg\n",
        "        output_embeddings_data[-num_new_tokens:] = output_embeddings_avg\n",
        "    print(f\"Resized tokenizer and embedding to {len(tokenizer)} tokens.\")\n",
        "\n",
        "def print_trainable_parameters(args, model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || \"\n",
        "        f\"all params: {all_param} || \"\n",
        "    )\n",
        "\n",
        "def get_last_checkpoint(checkpoint_dir):\n",
        "    if isdir(checkpoint_dir):\n",
        "        is_completed = exists(join(checkpoint_dir, 'completed'))\n",
        "        if is_completed: return None, True # already finished\n",
        "        max_step = 0\n",
        "        for filename in os.listdir(checkpoint_dir):\n",
        "            if isdir(join(checkpoint_dir, filename)) and filename.startswith('checkpoint'):\n",
        "                max_step = max(max_step, int(filename.replace('checkpoint-', '')))\n",
        "        if max_step == 0: return None, is_completed # training started, but no checkpoint\n",
        "        checkpoint_dir = join(checkpoint_dir, f'checkpoint-{max_step}')\n",
        "        print(f\"Found a previous checkpoint at: {checkpoint_dir}\")\n",
        "        return checkpoint_dir, is_completed # checkpoint found!\n",
        "    return None, False # first training\n"
      ],
      "metadata": {
        "id": "c15kggVaeDpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import login\n",
        "# from google.colab import userdata\n",
        "# HUGGINGFACE_TOKEN = userdata.get('HUGGINGFACE_TOKEN')\n",
        "# login(token=HUGGINGFACE_TOKEN)"
      ],
      "metadata": {
        "id": "pQGpsBbQJnrB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "yBMNCXKpPCZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unaries=pd.read_csv('/content/drive/MyDrive/Fine-tuning-llms/data_unaries.csv')\n",
        "oxides=pd.read_csv('/content/drive/MyDrive/Fine-tuning-llms/data_oxides.csv')"
      ],
      "metadata": {
        "id": "FzzoQ1JlJyBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y6ty9lW4Mxuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\""
      ],
      "metadata": {
        "id": "K4FFf2KDM22e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_args = ModelArguments(model_name_or_path=\"TinyLlama/TinyLlama_v1.1_math_code\")"
      ],
      "metadata": {
        "id": "ds6f6w6vf_40"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_args = DataArguments(eval_dataset_size=1024,\n",
        "                        max_train_samples=10,\n",
        "                        max_eval_samples=10,\n",
        "                        source_max_len=4096,\n",
        "                        target_max_len=1024,\n",
        "                        dataset='/content/drive/MyDrive/Fine-tuning-llms/qe_input_dataset.csv',\n",
        "                        dataset_format=\"input-output\")"
      ],
      "metadata": {
        "id": "hnVUaHI0k9bN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(train_on_source = False,\n",
        "                                  optim = 'adamw_torch',\n",
        "                                  per_device_train_batch_size=16,\n",
        "                                  gradient_accumulation_steps=1,\n",
        "                                  weight_decay=0,\n",
        "                                  learning_rate=0.002,\n",
        "                                  max_grad_norm=0.3,\n",
        "                                  gradient_checkpointing=True,\n",
        "                                  do_train=True,\n",
        "                                  lr_scheduler_type='constant',\n",
        "                                  warmup_ratio=0.03,\n",
        "                                  group_by_length=True,\n",
        "                                  save_strategy='steps',\n",
        "                                  save_steps=250,\n",
        "                                  save_total_limit=40,\n",
        "                                  report_to='none',\n",
        "                                  logging_steps=10)"
      ],
      "metadata": {
        "id": "njs-H4sAk7Bh"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generation_args = GenerationArguments(max_new_tokens = 1024,\n",
        "                                      do_sample = False,\n",
        "                                      num_beams = 1,\n",
        "                                      num_beam_groups = 1,\n",
        "                                      penalty_alpha = None,\n",
        "                                      use_cache = True,\n",
        "                                      temperature = 0.01,\n",
        "                                      top_k = 50,\n",
        "                                      top_p = 1.0,\n",
        "                                      diversity_penalty = 1.0,\n",
        "                                      repetition_penalty = 1.0,\n",
        "                                      length_penalty = 1.0,\n",
        "                                      no_repeat_ngram_size = 0)"
      ],
      "metadata": {
        "id": "e3ZsNwPNmc5r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "args = argparse.Namespace(\n",
        "        **vars(model_args), **vars(data_args), **vars(training_args)\n",
        "    )"
      ],
      "metadata": {
        "id": "sTDYzwMdtr2s"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 123"
      ],
      "metadata": {
        "id": "0pZm8jR3tdCt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args.generation_config = transformers.GenerationConfig(**vars(generation_args))\n",
        "checkpoint_dir, completed_training = get_last_checkpoint(args.output_dir)\n",
        "if completed_training:\n",
        "    print('Detected that training was already completed!')\n",
        "\n",
        "model, tokenizer = get_accelerate_model(args, checkpoint_dir)\n",
        "\n",
        "model.config.use_cache = False\n",
        "print('loaded model')\n",
        "set_seed(seed)\n",
        "data_module = make_data_module(tokenizer=tokenizer, args=args)\n",
        "trainer = Seq2SeqTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=training_args,\n",
        "        **{k:v for k,v in data_module.items() if k != 'predict_dataset'},\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        },
        "id": "-YtWeIQggAGV",
        "outputId": "18547836-a539-45b0-f5e8-ad8008b4fdb7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:627: UserWarning: `num_beams` is set to 1. However, `diversity_penalty` is set to `1.0` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `diversity_penalty`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_last_checkpoint' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-44780421e3af>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneration_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerationConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mvars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgeneration_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcheckpoint_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompleted_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_last_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcompleted_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Detected that training was already completed!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'get_last_checkpoint' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifying the datatypes and parameter counts before training.\n",
        "print_trainable_parameters(args, model)\n",
        "dtypes = {}\n",
        "for _, p in model.named_parameters():\n",
        "    dtype = p.dtype\n",
        "    if dtype not in dtypes: dtypes[dtype] = 0\n",
        "    dtypes[dtype] += p.numel()\n",
        "total = 0\n",
        "for k, v in dtypes.items():\n",
        "    total+= v\n",
        "for k, v in dtypes.items():\n",
        "    print(k, v, v/total)"
      ],
      "metadata": {
        "id": "XM_vuV5OuF_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import logging\n",
        "# logger = logging.getLogger(__name__)\n",
        "args.run_name = 'test_run'\n",
        "all_metrics = {\"run_name\": args.run_name}\n",
        "\n",
        "# Training\n",
        "if args.do_train:\n",
        "    # logger.info(\"*** Train ***\")\n",
        "    # Note: `resume_from_checkpoint` not supported for adapter checkpoints by HF.\n",
        "    # Currently adapter checkpoint is reloaded as expected but optimizer/scheduler states are not.\n",
        "    train_result = trainer.train()\n",
        "    metrics = train_result.metrics\n",
        "    # trainer.log_metrics(\"train\", metrics)\n",
        "    # trainer.save_metrics(\"train\", metrics)\n",
        "    trainer.save_state()\n",
        "    all_metrics.update(metrics)\n",
        "    # Evaluation\n",
        "if args.do_eval:\n",
        "    # logger.info(\"*** Evaluate ***\")\n",
        "    metrics = trainer.evaluate(metric_key_prefix=\"eval\")\n",
        "    trainer.log_metrics(\"eval\", metrics)\n",
        "    trainer.save_metrics(\"eval\", metrics)\n",
        "    all_metrics.update(metrics)\n",
        "    # Prediction\n",
        "if args.do_predict:\n",
        "    # logger.info(\"*** Predict ***\")\n",
        "    prediction_output = trainer.predict(test_dataset=data_module['predict_dataset'],metric_key_prefix=\"predict\")\n",
        "    prediction_metrics = prediction_output.metrics\n",
        "    predictions = prediction_output.predictions\n",
        "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
        "    predictions = tokenizer.batch_decode(\n",
        "            predictions, skip_special_tokens=True, clean_up_tokenization_spaces=True\n",
        "        )\n",
        "    with open(os.path.join(args.output_dir, 'predictions.jsonl'), 'w') as fout:\n",
        "        for i, example in enumerate(data_module['predict_dataset']):\n",
        "            example['prediction_with_input'] = predictions[i].strip()\n",
        "            example['prediction'] = predictions[i].replace(example['input'], '').strip()\n",
        "            fout.write(json.dumps(example) + '\\n')\n",
        "    print(prediction_metrics)\n",
        "    trainer.log_metrics(\"predict\", prediction_metrics)\n",
        "    trainer.save_metrics(\"predict\", prediction_metrics)\n",
        "    all_metrics.update(prediction_metrics)\n",
        "\n",
        "if (args.do_train or args.do_eval or args.do_predict):\n",
        "    with open(os.path.join(args.output_dir, \"metrics.json\"), \"w\") as fout:\n",
        "        fout.write(json.dumps(all_metrics))"
      ],
      "metadata": {
        "id": "yWpl-bYBgAJA"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5z-EPhkv8Ai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eTk2iFsmv8DL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mnpw81pLv8F-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "V6R9P_WCgAMI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}