{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SQoQqglcJUOq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "from google.colab import drive\n",
        "import random\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q datasets\n",
        "!pip install -q accelerate\n",
        "!pip install -q peft\n",
        "!pip install -q bitsandbytes\n",
        "!pip install -q trl\n",
        "!pip install --upgrade -q trl"
      ],
      "metadata": {
        "id": "F-um9DiKJj8X"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True"
      ],
      "metadata": {
        "id": "cxPrauGTk918"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxJvNEL8WBB0",
        "outputId": "9cb89482-6520-4c88-a816-e4f5798cbab7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "vmgbXSD3jkAa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id=\"TinyLlama/TinyLlama_v1.1_math_code\""
      ],
      "metadata": {
        "id": "J8fAVfeTWBFQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your dataset (example with input-output pairs)\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "train_dataset = load_dataset('csv', data_files=['/content/drive/MyDrive/Fine-tuning-llms/qe_input_dataset.csv'],\\\n",
        "                       column_names=['input','output'], split='train[:80%]')\n",
        "eval_dataset = load_dataset('csv', data_files=['/content/drive/MyDrive/Fine-tuning-llms/qe_input_dataset.csv'],\\\n",
        "                       column_names=['input','output'], split='train[80%:]')\n",
        "\n",
        "# Load the tokenizer for your model (LLaMA tokenizer, for example)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Define a function to tokenize your dataset\n",
        "def preprocess_function(examples):\n",
        "    texts=[]\n",
        "    for inputs,outputs in zip(examples['input'],examples['output']):\n",
        "        input_text = 'user: '+ inputs + 'assistant: '+ outputs + tokenizer.eos_token\n",
        "        texts.append(input_text)\n",
        "\n",
        "    tokenized_inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=2048,return_tensors = \"pt\")\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Apply the preprocessing function to your dataset\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Remove the original columns (instruction, output) as we now have input_ids and labels\n",
        "train_dataset = train_dataset.remove_columns(['input', 'output'])\n",
        "eval_dataset = eval_dataset.remove_columns(['input', 'output'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_d13bPuWBIP",
        "outputId": "97b9fbb5-55a4-45cc-b619-690dcf0f32a1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "VVC7HQJAZIpp"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, set_seed\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"TinyLlama/TinyLlama_v1.1_math_code\",\n",
        "        device_map = 'auto')\n",
        "\n",
        "\n",
        "base_model.config.use_cache = False\n",
        "set_seed(123)"
      ],
      "metadata": {
        "id": "BD4c_fq7WBK9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# LoRA config based on QLoRA paper\n",
        "peft_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.01,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")"
      ],
      "metadata": {
        "id": "CjQW_x0TX5ue"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training, get_peft_model\n",
        "from transformers import TrainingArguments\n",
        "from transformers import BitsAndBytesConfig\n",
        "import torch\n",
        "\n",
        "# # bitsandbytes\n",
        "use_4bit = True\n",
        "bnb_4bit_compute_dtype = \"float16\"\n",
        "bnb_4bit_quant_type = \"nf4\"\n",
        "use_nested_quant = False\n",
        "device_map = \"auto\"\n",
        "\n",
        "compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\n",
        "print(compute_dtype)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=use_4bit,\n",
        "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
        "    bnb_4bit_compute_dtype=compute_dtype,\n",
        "    bnb_4bit_use_double_quant=use_nested_quant\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(base_model)\n",
        "model = get_peft_model(model, peft_config)\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    output_dir='/content/drive/MyDrive/Fine-tuning-llms/test_output',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    max_steps = 60,\n",
        "    weight_decay=0,\n",
        "    save_strategy='epoch',\n",
        "    save_steps=20,\n",
        "    warmup_ratio=0.09,\n",
        "    learning_rate=0.002,\n",
        "    max_grad_norm=0.3,\n",
        "    gradient_checkpointing=True,\n",
        "    per_device_train_batch_size=8,\n",
        "    gradient_accumulation_steps=3,\n",
        "    per_device_eval_batch_size=8,\n",
        "    # num_train_epochs=1,\n",
        "    logging_steps=2,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "\n",
        ")\n",
        "from transformers import Trainer\n",
        "trainer = Trainer(\n",
        "        model=model,\n",
        "        args=train_args,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        # eval_dataset=eval_dataset,\n",
        "        data_collator=data_collator,\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9vDx9XRX5xV",
        "outputId": "cd0b4ba4-4b9d-481c-f771-388cc207d5cd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.float16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(args, model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || \"\n",
        "        f\"all params: {all_param} || \"\n",
        "    )\n",
        "\n",
        "# Verifying the datatypes and parameter counts before training.\n",
        "print_trainable_parameters(train_args, model)\n",
        "dtypes = {}\n",
        "for _, p in model.named_parameters():\n",
        "    dtype = p.dtype\n",
        "    if dtype not in dtypes: dtypes[dtype] = 0\n",
        "    dtypes[dtype] += p.numel()\n",
        "total = 0\n",
        "for k, v in dtypes.items():\n",
        "    total+= v\n",
        "for k, v in dtypes.items():\n",
        "    print(k, v, v/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFIylohbieBi",
        "outputId": "fb9671df-976c-49bd-aa9f-d0a1bbc567ee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 13160448 || all params: 1113208832 || \n",
            "torch.float32 1113208832 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "13160448/1113208832"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uEXBGKxwitam",
        "outputId": "1907f86f-58bf-4878-e2bd-8d2d993e3d37"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.011822083711243858"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "id": "9LRDlwPGYU4_",
        "outputId": "041a8255-513b-456f-aaa3-4e27e09f4ee7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:92: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='22' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22/60 03:50 < 07:18, 0.09 it/s, Epoch 0.08/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Fine-tuning-llms/test_output/model_tiny/\")\n",
        "peft_config.save_pretrained(\"/content/drive/MyDrive/Fine-tuning-llms/test_output/model_tiny/\")"
      ],
      "metadata": {
        "id": "YjzFv3eFYU7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XFuZkBhBXw1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7esC0xIlXw4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FT9uiYNFWBN0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0ls5Jde51KwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tiucfe681Kyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N49Dq3P81K1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sILmmLQvaEST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IdRLEPzJjE3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9mE22QeFjE6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kTFo8TC2jE9X"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}